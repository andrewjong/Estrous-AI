{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import (VGG16, InceptionV3, ResNet50, VGG19, Xception, InceptionResNetV2, DenseNet201, \n",
    "NASNetMobile, NASNetLarge, MobileNet)\n",
    "\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_flag = 1\n",
    "pool = None\n",
    "\n",
    "if model_flag == 1:\n",
    "    vgg_conv = VGG16(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "    vgg_conv.summary()\n",
    "elif model_flag == 2:\n",
    "    inc_conv = InceptionV3(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 3:\n",
    "    resnet_conv = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 4:\n",
    "    vgg19_conv = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 5:\n",
    "    xcep_conv = Xception(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(299,299,3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 6:\n",
    "    inc_res_conv = InceptionResNetV2(weights='imagenet',\n",
    "                      include_top=False,\n",
    "                      input_shape=(299,299,3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 7:\n",
    "    dense201_conv = DenseNet201(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 8:\n",
    "    nasnet_conv = NASNetMobile(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 9:\n",
    "    nasnet_conv = NASNetLarge(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(331, 331, 3),\n",
    "                      pooling=pool)\n",
    "elif model_flag == 10:\n",
    "    mobilenet_conv = MobileNet(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3),\n",
    "                      pooling=pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 987 images belonging to 4 classes.\n",
      "Obtaining training data\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-81102c691395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfeatures_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel_flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfeatures_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minc_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/eai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m       return training_arrays.predict_loop(\n\u001b[0;32m-> 1493\u001b[0;31m           self, x, batch_size=batch_size, verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/eai/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, inputs, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/eai/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/eai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dir = 'data/4_class_11/train'\n",
    "validation_dir = 'data/4_class_11/val'\n",
    "\n",
    "nTrain = 987\n",
    "nVal = 211\n",
    "nClasses = 4\n",
    "\n",
    "if model_flag == 5 or model_flag == 6:\n",
    "    datagen = ImageDataGenerator(rescale=1./255,data_format=\"channels_last\")\n",
    "    target_sz = 299\n",
    "elif model_flag == 9:\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    target_sz = 331\n",
    "else:\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    target_sz = 224\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "if model_flag == 1:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 512\n",
    "    model_name = 'VGG16'\n",
    "elif model_flag == 2:\n",
    "    d1 = 5\n",
    "    d2 = 5\n",
    "    d3 = 2048\n",
    "    model_name = 'InceptionV3'\n",
    "elif model_flag == 3:\n",
    "    d1 = 1\n",
    "    d2 = 1\n",
    "    d3 = 2048\n",
    "    model_name = 'Resnet50'\n",
    "elif model_flag == 4:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 512\n",
    "    model_name = 'VGG19'\n",
    "elif model_flag == 5:\n",
    "    d1 = 10\n",
    "    d2 = 10\n",
    "    d3 = 2048\n",
    "    model_name = 'Xception'\n",
    "elif model_flag == 6:\n",
    "    d1 = 8\n",
    "    d2 = 8\n",
    "    d3 = 1536\n",
    "    model_name = 'InceptionResNetV2'\n",
    "elif model_flag == 7:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 1920\n",
    "    model_name = 'DenseNet201'\n",
    "elif model_flag == 8:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 1056\n",
    "    model_name = 'NASNetMobile'\n",
    "elif model_flag == 9:\n",
    "    d1 = 11\n",
    "    d2 = 11\n",
    "    d3 = 4032\n",
    "    model_name = 'NASNetLarge'\n",
    "elif model_flag == 10:\n",
    "    d1 = 7\n",
    "    d2 = 7\n",
    "    d3 = 1024\n",
    "    model_name = 'MobileNet'\n",
    "    \n",
    "if pool is not None:\n",
    "    d1 = 1\n",
    "    d2 = 1\n",
    "    \n",
    "train_features = np.zeros(shape=(nTrain, d1, d2, d3))\n",
    "\n",
    "train_labels = np.zeros(shape=(nTrain,nClasses))\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(target_sz, target_sz),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "print('Obtaining training data')\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in train_generator:\n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "    if model_flag == 1:\n",
    "        features_batch = vgg_conv.predict(inputs_batch)\n",
    "    elif model_flag == 2:\n",
    "        features_batch = inc_conv.predict(inputs_batch)\n",
    "    elif model_flag == 3:\n",
    "        features_batch = resnet_conv.predict(inputs_batch)\n",
    "    elif model_flag == 4:\n",
    "        features_batch = vgg19_conv.predict(inputs_batch)        \n",
    "    elif model_flag == 5:\n",
    "        features_batch = xcep_conv.predict(inputs_batch) \n",
    "    elif model_flag == 6:\n",
    "        features_batch = inc_res_conv.predict(inputs_batch) \n",
    "    elif model_flag == 7:\n",
    "        features_batch = dense201_conv.predict(inputs_batch) \n",
    "    elif model_flag == 8 or model_flag == 9:\n",
    "        features_batch = nasnet_conv.predict(inputs_batch) \n",
    "    elif model_flag == 10:\n",
    "        features_batch = mobilenet_conv.predict(inputs_batch) \n",
    "    #print(features_batch.shape)\n",
    "    b_sz = features_batch.shape[0]\n",
    "\n",
    "    if features_batch.shape[0] < batch_size:\n",
    "        train_features[i * batch_size : i * batch_size + b_sz] = np.reshape(features_batch, (b_sz,d1,d2,d3))\n",
    "        train_labels[i * batch_size : i * batch_size + b_sz] = labels_batch\n",
    "    else:\n",
    "        train_features[i * batch_size : (i + 1) * batch_size] = np.reshape(features_batch, (batch_size,d1,d2,d3))\n",
    "        train_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "    i += 1\n",
    "    if i * batch_size >= nTrain:\n",
    "        break\n",
    "\n",
    "\n",
    "train_features = np.reshape(train_features, (nTrain, d1 * d2 * d3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = np.zeros(shape=(nVal, d1, d2, d3))\n",
    "\n",
    "validation_labels = np.zeros(shape=(nVal,2))\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(target_sz, target_sz),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)\n",
    "\n",
    "print('Obtaining validation data')\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in validation_generator:\n",
    "    if model_flag == 1:\n",
    "        features_batch = vgg_conv.predict(inputs_batch)\n",
    "    elif model_flag == 2:\n",
    "        features_batch = inc_conv.predict(inputs_batch)\n",
    "    elif model_flag == 3:\n",
    "        features_batch = resnet_conv.predict(inputs_batch)    \n",
    "    elif model_flag == 4:\n",
    "        features_batch = vgg19_conv.predict(inputs_batch)        \n",
    "    elif model_flag == 5:\n",
    "        features_batch = xcep_conv.predict(inputs_batch) \n",
    "    elif model_flag == 6:\n",
    "        features_batch = inc_res_conv.predict(inputs_batch) \n",
    "    elif model_flag == 7:\n",
    "        features_batch = dense201_conv.predict(inputs_batch) \n",
    "    elif model_flag == 8 or model_flag == 9:\n",
    "        features_batch = nasnet_conv.predict(inputs_batch) \n",
    "    elif model_flag == 10:\n",
    "        features_batch = mobilenet_conv.predict(inputs_batch) \n",
    "    \n",
    "    b_sz = features_batch.shape[0]\n",
    "\n",
    "    if features_batch.shape[0] < batch_size:\n",
    "        validation_features[i * batch_size : i * batch_size + b_sz] = np.reshape(features_batch, (b_sz,d1,d2,d3))\n",
    "        validation_labels[i * batch_size : i * batch_size + b_sz] = labels_batch\n",
    "    else:\n",
    "        validation_features[i * batch_size : (i + 1) * batch_size] = np.reshape(features_batch, (batch_size,d1,d2,d3))\n",
    "        validation_labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        \n",
    "    i += 1\n",
    "    if i * batch_size >= nVal:\n",
    "        break\n",
    "\n",
    "validation_features = np.reshape(validation_features, (nVal, d1 * d2 * d3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers, initializers, regularizers, callbacks\n",
    "\n",
    "activ = 'relu'\n",
    "loss = 'categorical_crossentropy'\n",
    "optim = 1\n",
    "init = initializers.random_normal()#initializers.glorot_normal()\n",
    "bnorm_flag = False\n",
    "model_save_folder = 'saved_models'\n",
    "if not os.path.isdir(model_save_folder):\n",
    "    os.mkdir(model_save_folder)\n",
    "\n",
    "itr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model():\n",
    "    def __init__(self, drpout, init, activ, bnorm_flag, lrt, loss, optim, decay_rt, decay_steps, \n",
    "                 batch_size, n_epochs, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "        self.drpout = drpout\n",
    "        self.init = init\n",
    "        self.activ = activ\n",
    "        self.bnorm_flag = bnorm_flag\n",
    "        self.lrt = lrt\n",
    "        self.loss = loss\n",
    "        self.optim = optim\n",
    "        self.decay_rt = decay_rt\n",
    "        self.decay_steps = np.max([np.round(decay_steps*n_epochs), 1])\n",
    "        print('decay step percentage', decay_steps)\n",
    "        print('decay steps', self.decay_steps)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.val_X = val_X\n",
    "        self.val_y = val_y        \n",
    "        self.test_X = test_X\n",
    "        self.test_y = test_y\n",
    "        self.model = self.set_up_model()\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        \n",
    "        model = models.Sequential()\n",
    "        \n",
    "        '''\n",
    "        if self.init == 0:\n",
    "            init = initializers.RandomNormal()\n",
    "        else:\n",
    "            init = initializers.glorot_normal()\n",
    "        \n",
    "        activ = 'relu'\n",
    "        if self.activ == 1:\n",
    "            activ = 'relu'\n",
    "        elif self.activ == 2:\n",
    "            activ = 'tanh'\n",
    "        elif self.activ == 3:\n",
    "            activ = 'sigmoid'\n",
    "            \n",
    "        if self.loss == 1:\n",
    "            loss = 'categorical_crossentropy'\n",
    "        elif self.loss == 2:\n",
    "            loss = 'mean_squared_error'\n",
    "        elif self.loss == 3:\n",
    "            loss = 'kullback_leibler_divergence'\n",
    "        elif self.loss == 4:\n",
    "            loss = 'categorical_hinge'\n",
    "        '''\n",
    "        \n",
    "        model.add(layers.Dense(d3, activation=self.activ, input_dim=d1 * d2 * d3, kernel_initializer=self.init))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        if self.bnorm_flag:\n",
    "            model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(self.drpout))\n",
    "        model.add(layers.Dense(2, activation='softmax', kernel_initializer=self.init))#, kernel_regularizer=regularizers.l2(0.01)))\n",
    "        \n",
    "        optim = optimizers.RMSprop(lr=self.lrt)\n",
    "        if self.optim == 1:\n",
    "            optim = optimizers.RMSprop(lr=self.lrt)\n",
    "        elif self.optim == 2:\n",
    "            optim = optimizers.Adam(lr=self.lrt)\n",
    "        elif self.optim == 3:\n",
    "            optim = optimizers.SGD(lr=self.lrt)\n",
    "            \n",
    "        model.compile(optimizer=optim, loss=loss, metrics=['acc'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def step_decay(self, epoch):\n",
    "            steps = self.decay_steps#np.floor(n_epochs/3)\n",
    "            lrate = self.lrt * (self.decay_rt)**np.floor(epoch/steps)\n",
    "            return lrate\n",
    "        \n",
    "    def train_model(self, save_folder):\n",
    "        global itr\n",
    "        earlystop = EarlyStopping(patience=10)\n",
    "        checkpointer = callbacks.ModelCheckpoint(filepath=save_folder+'/weights'+str(itr)+'.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "        lrate=keras.callbacks.LearningRateScheduler(self.step_decay,verbose=1)\n",
    "        callbacks_list = [earlystop, lrate, checkpointer]\n",
    "        history = self.model.fit(self.train_X,\n",
    "                self.train_y,\n",
    "                epochs=self.n_epochs,\n",
    "                callbacks=callbacks_list,\n",
    "                batch_size=self.batch_size,\n",
    "                validation_data=(self.val_X,self.val_y))\n",
    "        #dscr_phrase = 'rmsprop' + str(lrt) + 'decay_' + str(decay_rt) + '_' + loss + 'glorot_normal_init'# + 'dropout_' + str(drpout)\n",
    "        #model.save(model_name + '_mmode_nopreprocessing_' + dscr_phrase + '.h5')\n",
    "        self.model = keras.models.load_model(save_folder+'/weights'+str(itr)+'.hdf5')\n",
    "        \n",
    "    def model_evaluate(self, save_folder):\n",
    "        self.train_model(save_folder)\n",
    "        \n",
    "        evaluation = self.model.evaluate(self.test_X, self.test_y, batch_size=self.batch_size, verbose=0)\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_my_model(drpout, init, activ, bnorm_flag, lrt, loss, optim, decay_rt, decay_steps, \n",
    "                 batch_size, n_epochs, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    \n",
    "    model = my_model(drpout=drpout, init=init, activ=activ, bnorm_flag=bnorm_flag, lrt=lrt, loss=loss, \n",
    "                     optim=optim, decay_rt=decay_rt, decay_steps=decay_steps, batch_size=batch_size, \n",
    "                     n_epochs=n_epochs, train_X=train_X, train_y=train_y, val_X=val_X, val_y=val_y, \n",
    "                     test_X=test_X, test_y=test_y)\n",
    "    model_evaluation = model.model_evaluate(model_save_folder)\n",
    "    return model_evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds for hyper-parameters in model\n",
    "bounds = {'drpout': (0.0, 0.8),\n",
    "          'lrt': (1e-5, 1e-3),\n",
    "          'decay_rt': (1e-6, 1.0),\n",
    "          'decay_steps': (0.05, 0.5),\n",
    "          'batch_size': (16, 128),\n",
    "          'n_epochs': (20, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(drpout, lrt, decay_rt, decay_steps, batch_size, n_epochs):\n",
    "    global itr\n",
    "    itr += 1\n",
    "    print('iteration:',itr)\n",
    "    global all_params\n",
    "    params ={'drpout': drpout, 'decay_steps': decay_steps, 'n_epochs': n_epochs, 'lrt': lrt, 'batch_size': batch_size, 'decay_rt': decay_rt}\n",
    "    all_params.append(params)\n",
    "    evaluation = run_my_model(drpout=drpout, init=init, activ=activ, bnorm_flag=bnorm_flag, \n",
    "                              lrt=lrt, loss=loss, optim=optim, decay_rt=decay_rt, \n",
    "                                        decay_steps=decay_steps, batch_size=int(batch_size), n_epochs=int(n_epochs),\n",
    "                             train_X=train_features, train_y=train_labels, val_X=validation_features,\n",
    "                             val_y=validation_labels, test_X=validation_features, test_y=validation_labels)\n",
    "    print(\"LOSS:\\t{0} \\t ACCURACY:\\t{1}\".format(evaluation[0], evaluation[1]))\n",
    "    print(evaluation)\n",
    "    return evaluation[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bayes_opt\n",
    "\n",
    "itr = 0\n",
    "all_params = []\n",
    "opt_model = bayes_opt.BayesianOptimization(f, bounds)\n",
    "opt_model.maximize(n_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_itr = 'unknown'\n",
    "for i in range(len(all_params)):\n",
    "    if all_params[i]==opt_model.res['max']['max_params']:\n",
    "        best_itr = i\n",
    "\n",
    "print(best_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results:', opt_model.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(model_save_folder+'/weights'+str(best_itr+1)+'.hdf5',model_save_folder+'/best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "fieldnames = []\n",
    "for key, value in opt_model.res['all']['params'][0].items():\n",
    "    fieldnames.append(key)\n",
    "fieldnames.append('values')\n",
    "\n",
    "print(fieldnames)\n",
    "\n",
    "with open('parameters.csv', 'w') as csvfile:\n",
    "    row = opt_model.res['all']['params'][0]\n",
    "    row['values'] = opt_model.res['all']['values'][0]\n",
    "    row['test'] = 1\n",
    "    fieldnames = []\n",
    "    for key, value in row.items():\n",
    "        fieldnames.append(key)\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for itr in range(1,len(opt_model.res['all']['params'])):\n",
    "        row = opt_model.res['all']['params'][itr]\n",
    "        row['values'] = opt_model.res['all']['values'][itr]\n",
    "        row['test'] = 1\n",
    "        writer.writerow(row)\n",
    "#    writer.writerow({'first_name': 'Lovely', 'last_name': 'Spam'})\n",
    "#    writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('parameters.csv')\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "plt.figure()\n",
    "parallel_coordinates(data,'test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "fieldnames.remove('test')\n",
    "data.pop('test')\n",
    "cols = fieldnames\n",
    "x = [i for i, _ in enumerate(cols)]\n",
    "\n",
    "# Create (X-1) sublots along x axis\n",
    "fig, axes = plt.subplots(1, len(x)-1, sharey=False, figsize=(15,5))\n",
    "\n",
    "# Get min, max and range for each column\n",
    "# Normalize the data for each column\n",
    "min_max_range = {}\n",
    "for col in cols:\n",
    "    if not col == 'test':\n",
    "        min_max_range[col] = [data[col].min(), data[col].max(), np.ptp(data[col])]\n",
    "        data[col] = np.true_divide(data[col] - data[col].min(), np.ptp(data[col]))\n",
    "    else:\n",
    "        min_max_range[col] = [data[col].min(), data[col].max(), np.ptp(data[col])]\n",
    "        #data[col] = data[col] - data[col].min(), np.ptp(data[col]))\n",
    "print(data)\n",
    "print('data normalized')\n",
    "\n",
    "# Plot each row\n",
    "for i, ax in enumerate(axes):\n",
    "    for idx in data.index:\n",
    "        test_category = data.loc[idx, 'values']\n",
    "        ax.plot(x, data.loc[idx, cols])\n",
    "    ax.set_xlim([x[i], x[i+1]])\n",
    "\n",
    "print('rows plotted')\n",
    "    \n",
    "# Set the tick positions and labels on y axis for each plot\n",
    "# Tick positions based on normalised data\n",
    "# Tick labels are based on original data\n",
    "def set_ticks_for_axis(dim, ax, ticks):\n",
    "    min_val, max_val, val_range = min_max_range[cols[dim]]\n",
    "    step = val_range / float(ticks-1)\n",
    "    tick_labels = [round(min_val + step * i, 2) for i in range(ticks)]\n",
    "    norm_min = data[cols[dim]].min()\n",
    "    norm_range = np.ptp(data[cols[dim]])\n",
    "    norm_step = norm_range / float(ticks-1)\n",
    "    ticks = [round(norm_min + norm_step * i, 2) for i in range(ticks)]\n",
    "    ax.yaxis.set_ticks(ticks)\n",
    "    ax.set_yticklabels(tick_labels)\n",
    "\n",
    "for dim, ax in enumerate(axes):\n",
    "    ax.xaxis.set_major_locator(ticker.FixedLocator([dim]))\n",
    "    set_ticks_for_axis(dim, ax, ticks=6)\n",
    "    ax.set_xticklabels([cols[dim]])\n",
    "    \n",
    "\n",
    "# Move the final axis' ticks to the right-hand side\n",
    "ax = plt.twinx(axes[-1])\n",
    "dim = len(axes)\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator([x[-2], x[-1]]))\n",
    "set_ticks_for_axis(dim, ax, ticks=6)\n",
    "ax.set_xticklabels([cols[-2], cols[-1]])\n",
    "\n",
    "\n",
    "# Remove space between subplots\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "plt.title(\"Parameters\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
