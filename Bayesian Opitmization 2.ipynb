{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show more than one output in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# plot charts in our notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models as tvm\n",
    "import pretrainedmodels as ptm\n",
    "models = (\n",
    "    ptm.alexnet, # gets maximum recursion limit exceeded exceptions\n",
    "    ptm.se_resnet50,\n",
    "    ptm.se_resnet101,\n",
    "    ptm.inceptionresnetv2,\n",
    "    ptm.inceptionv4,\n",
    "    ptm.vgg16,\n",
    "    ptm.vgg19,\n",
    "    tvm.resnet101,\n",
    "    ptm.senet154,\n",
    "    ptm.nasnetalarge, # calculated output size is too small\n",
    ")\n",
    "\n",
    "domain = [{'name': 'batch_size', 'type': 'discrete', 'domain': (16, 24, 32, 48, 64)},]\n",
    "\n",
    "adam_domain = domain + [\n",
    "    {'name': 'adam_lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'adam_beta1', 'type': 'continuous', 'domain': (0.8, .99)},\n",
    "    {'name': 'adam_beta2', 'type': 'continuous', 'domain': (0.95, .9999)},\n",
    "    {'name': 'adam_wtdecay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]\n",
    "#default_input = [32, 0.001, 0.9, 0.999, 0] # TODO: have to figure out how to do this\n",
    "\n",
    "SGD_domain = domain + [\n",
    "    {'name': 'lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'momentum', 'type': 'continuous', 'domain': (0.5, .99)},\n",
    "    {'name': 'weight_decay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import src.utils as utils\n",
    "from src.trainable import Trainable\n",
    "import torch\n",
    "\n",
    "\n",
    "def build_model(model_fn):\n",
    "    print(model_fn)\n",
    "    if 'pretrainedmodels' in model_fn.__module__:\n",
    "        model = model_fn(num_classes=1000, pretrained='imagenet')\n",
    "    else:\n",
    "        model = model_fn(pretrained=True)\n",
    "    return model\n",
    "    \n",
    "\n",
    "def f(x):\n",
    "    \"\"\" Value function to minimize for bayesian optimization \"\"\"\n",
    "    val_acc = train(\n",
    "        batch_size=int(x[:,0]),\n",
    "        adam_lr=float(x[:,1]),\n",
    "        adam_b1=float(x[:,2]),\n",
    "        adam_b2=float(x[:,3]),\n",
    "        adam_wtdecay=float(x[:,4]),\n",
    "    )\n",
    "    \n",
    "    return val_acc\n",
    "\n",
    "\n",
    "CRITERION = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def make_key_from_params(params):\n",
    "    \"\"\" Makes a unique key (as a tuple) from a given list of parameters.\n",
    "    \"\"\"\n",
    "    return tuple(round(param, 10) for param in params)\n",
    "\n",
    "\n",
    "def train(batch_size, adam_lr, adam_b1, adam_b2, adam_wtdecay):\n",
    "    input_params = locals()  # the parameters will become the key\n",
    "    \n",
    "    \n",
    "    image_size = utils.determine_image_size(utils.get_model_name_from_fn(chosen_model))\n",
    "    dataloaders = utils.get_train_val_dataloaders(\n",
    "        datadir=data_dir,\n",
    "        val_proportion=0.15,\n",
    "        image_size=image_size, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model = build_model(chosen_model)\n",
    "    utils.fit_model_last_to_dataset(model, dataloaders['train'].dataset)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 adam_lr, (adam_b1, adam_b2), adam_wtdecay)\n",
    "    \n",
    "    trainable = Trainable(dataloaders, model, CRITERION, optimizer, outdir=outdir)\n",
    "    acc = trainable.train(50, early_stop_limit=6, verbose=False)\n",
    "    \n",
    "    # store this train iteration in a dictionary for later\n",
    "    global trainables\n",
    "    key = make_key_from_params(input_params)\n",
    "    trainables.append({key: trainable})\n",
    "    return acc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do BO on all models on both datasets. Git push at each stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def make_outdir_name(datadir, model_name, prepend=\"\", append=\"\"):\n",
    "    dataset_name = os.path.basename(datadir)\n",
    "    return os.path.join(prepend, dataset_name, model_name, append) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function alexnet at 0x7f573fc30e18>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train:  10%|â–‰         | 96/987 [00:16<02:35,  5.73images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "from predict import create_predictions\n",
    "import subprocess\n",
    "import sys\n",
    "sys.setrecursionlimit(1500)\n",
    "\n",
    "EXPERIMENTS_DIR = os.path.join(\"experiments\", \"bayes_opt_2\")\n",
    "\n",
    "for model_fn in models:\n",
    "    global chosen_model\n",
    "    chosen_model = model_fn\n",
    "    \n",
    "    for data_index, datadir in enumerate(('data/die_vs_all_tt', 'data/4_class_tt')):\n",
    "        global iteration  # keep track of our optimization iterations for directory output\n",
    "        iteration = 0   # but reset to 0 each train run\n",
    "        global data_dir\n",
    "        data_dir = datadir\n",
    "        # make an output directory using the model, dataset, and BO iteration\n",
    "        global outdir\n",
    "        outdir = make_outdir_name(datadir, utils.get_model_name_from_fn(chosen_model), \n",
    "                                  prepend=EXPERIMENTS_DIR,\n",
    "                                  append=str(iteration))\n",
    "        \n",
    "        # storage of all the trainables produced by BO\n",
    "        global trainables\n",
    "        trainables = {}\n",
    "        \n",
    "        try:\n",
    "            # define and optimize the problem\n",
    "            problem = BayesianOptimization(\n",
    "                f=f,\n",
    "                domain=domain,\n",
    "                maximize=True\n",
    "            )\n",
    "            problem.run_optimization(max_iter=10)\n",
    "            \n",
    "            # plot the results\n",
    "            problem.plot_acquisition()\n",
    "            problem.plot_convergence()\n",
    "            \n",
    "            # save the best trainable\n",
    "            best_params = problem.x_opt\n",
    "            key = make_key_from_params(best_params)\n",
    "            best_trainable = trainables[key]\n",
    "            best_trainable.save()\n",
    "            \n",
    "            # evalute on the test set using the best model\n",
    "            predictions_file = create_predictions(\n",
    "                outdir=outdir,\n",
    "                subset='test',\n",
    "                data_dir=data_dir,\n",
    "                model=best_trainable.model\n",
    "            )\n",
    "            create_all_metrics(predictions_file, outdir, \"test\")\n",
    "            \n",
    "        # if something bad happens, skip it so we can let the others run\n",
    "        except Exception as e:\n",
    "            print('Skipping because', e)\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        # commit & push only if we can connect to internet\n",
    "        subprocess.check_call(['git', 'add', 'experiments'])\n",
    "        subprocess.check_call(['git', 'commit', '-am', f'Results from {str(chosen_model).split()[1]} {datadir}'])\n",
    "        subprocess.check_call(['git', 'push'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = subprocess.check_call([\"spd-say\", \"Your code has finished running\"])\n",
    "_ = subprocess.check_call(['git', 'commit', '-am', \"BO final commit\"])\n",
    "_ = subprocess.check_call(['git', 'push'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
