{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# whether to commit and push to git after each optimization. intended for long runs\n",
    "PUSH_TO_GIT = False  \n",
    "\n",
    "# parent output directory\n",
    "EXPERIMENTS_DIR = os.path.join(\"experiments\", \"bayes_opt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer to use. Choose by commenting. (Intent is to use train different ones on differen t machines for efficiency.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "CHOSEN_OPTIMIZER = torch.optim.SGD\n",
    "# CHOSEN_OPTIMIZER = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of iterations for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 30\n",
    "EARLY_STOP = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to test for BO. All in the list will be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as tvm\n",
    "import pretrainedmodels as ptm\n",
    "\n",
    "# The models we will test\n",
    "MODELS = (\n",
    "    ptm.alexnet, # gets maximum recursion limit exceeded exceptions\n",
    "    ptm.se_resnet50,\n",
    "    ptm.se_resnet101,\n",
    "    ptm.inceptionresnetv2,\n",
    "    ptm.inceptionv4,\n",
    "    ptm.vgg16,\n",
    "    ptm.vgg19,\n",
    "    tvm.resnet101,\n",
    "    ptm.senet154,\n",
    "    ptm.nasnetalarge\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Make sure Jupyter shows all output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show more than one output in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# plot charts in our notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "sys.setrecursionlimit(3000) # for some reason AlexNet requires more recursive depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import src.utils as utils\n",
    "from src.trainable import Trainable\n",
    "import torch\n",
    "\n",
    "\n",
    "CRITERION = torch.nn.CrossEntropyLoss() # we'll always use CE for the loss function\n",
    " \n",
    "\n",
    "def train(params):\n",
    "    \"\"\"\n",
    "    Set up a trainable and train it using the given parameters.\n",
    "    \"\"\"\n",
    "    print(params_to_meta_dict(params))\n",
    "    batch_size, lr_factor, optim_params = parse_train_params(params)\n",
    "    \n",
    "    # make an output directory using the model, dataset, and BO iteration\n",
    "    outdir = make_outdir_name(data_dir, utils.get_model_name_from_fn(chosen_model), \n",
    "                              prepend=EXPERIMENTS_DIR,\n",
    "                              append=str(iteration))\n",
    "    \n",
    "    \n",
    "    image_size = utils.determine_image_size(utils.get_model_name_from_fn(chosen_model))\n",
    "    dataloaders = utils.get_train_val_dataloaders(\n",
    "        datadir=data_dir,\n",
    "        val_proportion=0.15,\n",
    "        image_size=image_size, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model = build_model(chosen_model)\n",
    "    utils.fit_model_last_to_dataset(model, dataloaders['train'].dataset)\n",
    "    \n",
    "    \n",
    "    optimizer = CHOSEN_OPTIMIZER(model.parameters(), *optim_params)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=lr_factor, \n",
    "        patience=1\n",
    "    )\n",
    "    \n",
    "    trainable = Trainable(dataloaders, model, CRITERION, optimizer, lr_scheduler, outdir=outdir)\n",
    "    trainable.train(num_epochs=NUM_TRAIN_EPOCHS, early_stop_limit=EARLY_STOP, verbose=False)\n",
    "\n",
    "    return trainable\n",
    "\n",
    "\n",
    "def parse_train_params(params):\n",
    "    \"\"\"\n",
    "    Parses train parameters by converting batch size to an int, and the betas to a tuple for Adam.\n",
    "    \"\"\"\n",
    "    batch_size, lr_factor, optim_params = int(params[0]), params[1], params[2:]\n",
    "    if CHOSEN_OPTIMIZER is torch.optim.Adam:  # turn b1 and b2 into a tuple\n",
    "        optim_params = optim_params[:3] + tuple(optim_params[3:5]) + optim_params[5:]\n",
    "    return batch_size, lr_factor, optim_params\n",
    "\n",
    "    \n",
    "def build_model(model_fn):\n",
    "    \"\"\"\n",
    "    Build a pretrained model class from a model function. Passes \n",
    "    in the appropriate pretrained arg based on the model function's \n",
    "    parent module.\n",
    "    \"\"\"\n",
    "    if 'pretrainedmodels' in model_fn.__module__:\n",
    "        model = model_fn(num_classes=1000, pretrained='imagenet')\n",
    "    else:\n",
    "        model = model_fn(pretrained=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_key_from_params(params):\n",
    "    \"\"\"\n",
    "    Makes a unique key (as a tuple) from a given list of parameters.\n",
    "    For storing associated Trainable objects.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tuple(round(param, 10) for param in params)\n",
    "\n",
    "\n",
    "def make_outdir_name(datadir, model_name, prepend=\"\", append=\"\"):\n",
    "    \"\"\"\n",
    "    Make the output directory name based on dataset, model name, and any extra info.\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.basename(datadir)\n",
    "    return os.path.join(prepend, dataset_name, model_name, append) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_DOMAIN = [\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (1, 4, 8, 12)},  # DEBUG for GPUs with insufficient memory\n",
    "#     {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 24, 32, 48, 64)},\n",
    "    {'name': 'lr_decay', 'type': 'continuous', 'domain': (0.03, 0.3)},\n",
    "]\n",
    "# BASE_DOMAIN = \n",
    "\n",
    "ADAM_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'adam_lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'adam_beta1', 'type': 'continuous', 'domain': (0.8, .99)},\n",
    "    {'name': 'adam_beta2', 'type': 'continuous', 'domain': (0.95, .9999)},\n",
    "    {'name': 'adam_wtdecay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]\n",
    "# TODO: have to figure out how to set a starting default\n",
    "# default_input = [32, 0.001, 0.9, 0.999, 0] \n",
    "\n",
    "SGD_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'momentum', 'type': 'continuous', 'domain': (0.5, .99)},\n",
    "    {'name': 'weight_decay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" Value function to maximize for bayesian optimization \"\"\"\n",
    "    params = x.flatten()\n",
    "    \n",
    "    trainable = train(params)\n",
    "    val_acc = trainable.best_val_accuracy\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do BO on all models on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "from predict import create_predictions\n",
    "from metrics import create_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_globals(datadir):\n",
    "    global iteration  # keep track of our optimization iterations for directory output\n",
    "    iteration = 0   # but reset to 0 each train run\n",
    "    global data_dir\n",
    "    data_dir = datadir\n",
    "    \n",
    "    # reset the global trainables produced by BO\n",
    "    global trainables\n",
    "    trainables = {}\n",
    "\n",
    "def get_domain():\n",
    "    return ADAM_DOMAIN if CHOSEN_OPTIMIZER is torch.optim.Adam else SGD_DOMAIN\n",
    "    \n",
    "def perform_bayesian_optimization():\n",
    "    \"\"\"\n",
    "    Construct the problem and run the optimization.\n",
    "    \"\"\"\n",
    "    domain = get_domain()\n",
    "    problem = BayesianOptimization(\n",
    "        f=f,\n",
    "        domain=domain,\n",
    "        maximize=True\n",
    "    )\n",
    "    problem.run_optimization(max_iter=MAX_ITERATIONS)\n",
    "    return problem\n",
    "\n",
    "def plot_bo_results(problem):\n",
    "    \"\"\"\n",
    "    Graph the acquisition function and convergence\n",
    "    \"\"\"\n",
    "    print('Best params:', problem.opt_x)\n",
    "    problem.plot_acquisition()\n",
    "    problem.plot_convergence()\n",
    "    \n",
    "    \n",
    "def params_to_meta_dict(params):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of named parameters\n",
    "    \"\"\"\n",
    "    domain = get_domain()\n",
    "    meta = { d['name']: params[i] for i, d in enumerate(domain)}\n",
    "    return meta\n",
    "\n",
    "\n",
    "def generate_test_metrics(trainable):\n",
    "    \"\"\"\n",
    "    Create an itemized predictions file and metrics for the test set.\n",
    "    \"\"\"\n",
    "    predictions_file = create_predictions(\n",
    "        outdir=trainable.outdir,\n",
    "        subset='test',\n",
    "        data_dir=data_dir,\n",
    "        model=best_trainable.model\n",
    "    )\n",
    "    create_all_metrics(predictions_file, trainable.outdir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet data\\die_vs_all_tt\n",
      "{'batch_size': 1.0, 'lr_decay': 0.16699831689724726, 'lr': 0.042715851890708985, 'momentum': 0.9145557761910726, 'weight_decay': 0.17517534571112614}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping because 'Trainable' object has no attribute 'verbose'\n",
      "alexnet data\\4_class_tt\n",
      "{'batch_size': 12.0, 'lr_decay': 0.08625439462524992, 'lr': 0.03512263667603768, 'momentum': 0.5870795087047069, 'weight_decay': 0.45984973142176566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1, Train, best val=0.000000:   0%|                                                  | 0/1017 [00:00<?, ?images/s]"
     ]
    }
   ],
   "source": [
    "for model_fn in MODELS:  # iterate over all models\n",
    "    # set the model\n",
    "    global chosen_model\n",
    "    chosen_model = model_fn\n",
    "    \n",
    "    # iterate over both binary and quaternary datasets\n",
    "    for data_dir in (\n",
    "        os.path.join('data', 'die_vs_all_tt'), \n",
    "        os.path.join('data', '4_class_tt')\n",
    "    ):\n",
    "        print(utils.get_model_name_from_fn(chosen_model), data_dir)\n",
    "        reset_globals(data_dir)  # reset some globals used for iteration tracking\n",
    "        \n",
    "        try:\n",
    "            # define and optimize the problem\n",
    "            optimized = perform_bayesian_optimization()\n",
    "            # plot the results\n",
    "            plot_bo_results(optimized)\n",
    "            # get and save the best trainable\n",
    "            best_params = optimized.x_opt.flatten()\n",
    "            best_trainable = train(best_params)\n",
    "            best_trainable.save(extra_meta=params_to_meta_dict(best_params))\n",
    "            # evalute on the test set using the best model\n",
    "            generate_test_metrics(best_trainable)\n",
    "            \n",
    "        # if something bad happens, skip it so we can let the others run\n",
    "        except Exception as e:\n",
    "            print('Skipping because', e)\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        # commit & push only if we can connect to internet\n",
    "        if PUSH_TO_GIT:\n",
    "            subprocess.check_call(['git', 'add', 'experiments'])\n",
    "            subprocess.check_call(['git', 'commit', '-am', \n",
    "                                   f'Results from {utils.get_model_name_from_fn(chosen_model)} {data_dir}'])\n",
    "            subprocess.check_call(['git', 'push'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Commit and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_TO_GIT:\n",
    "    import time\n",
    "    time.sleep(120) # wait for two minutes to let everything rendering\n",
    "    _ = subprocess.check_call([\"spd-say\", \"Your code has finished running\"])\n",
    "    _ = subprocess.check_call(['git', 'commit', '-am', \"BO final commit\"])\n",
    "    _ = subprocess.check_call(['git', 'push'])\n",
    "    \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
