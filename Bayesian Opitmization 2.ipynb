{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# whether to commit and push to git after each optimization. intended for long runs\n",
    "PUSH_TO_GIT = True\n",
    "\n",
    "# parent output directory\n",
    "EXPERIMENTS_DIR = os.path.join(\"experiments\", \"bayes_opt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization and train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10     # max iterations for bayesian optimization\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 30   # total number of epochs tot rain for\n",
    "EARLY_STOP = 7          # give up training if validation accuracy doesn't improve after this many epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer to use. Choose by commenting. Supposedly by Wilson et al. 2018, SGD generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "OPTIMIZERS = (\n",
    "    optim.Adam,\n",
    "    optim.SGD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to test for BO. All in the list will be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as tvm\n",
    "import pretrainedmodels as ptm\n",
    "\n",
    "# The models we will test\n",
    "MODELS = (\n",
    "    ptm.alexnet, # gets maximum recursion limit exceeded exceptions\n",
    "#     se_resnext101_32x4d, # input size doesn't work\n",
    "    ptm.dpn98,\n",
    "    ptm.se_resnet50,\n",
    "    ptm.se_resnet101,\n",
    "    ptm.inceptionresnetv2,\n",
    "    ptm.inceptionv4,\n",
    "    ptm.vgg16,\n",
    "    ptm.vgg19,\n",
    "    tvm.resnet101,\n",
    "    tvm.resnet152,\n",
    "    ptm.senet154,\n",
    "    ptm.nasnetalarge\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Make sure Jupyter shows all output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show more than one output in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# plot charts in our notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import subprocess  # for command line control, git\n",
    "import sys\n",
    "sys.setrecursionlimit(3000) # for some reason AlexNet requires more recursive depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import src.utils as utils\n",
    "from src.trainable import Trainable\n",
    "import torch\n",
    "\n",
    "\n",
    "CRITERION = torch.nn.CrossEntropyLoss() # we'll always use CE for the loss function\n",
    " \n",
    "\n",
    "def train(params):\n",
    "    \"\"\"\n",
    "    Set up a trainable and train it using the given parameters.\n",
    "    \"\"\"\n",
    "#     print(params_to_meta_dict(params))  # print the input\n",
    "    print('Iteration:', iteration, end=', ')\n",
    "    batch_size, lr_factor, optim_params = parse_train_params(params)\n",
    "    \n",
    "    # make an output directory using the dataset name, model name, and BO iteration\n",
    "    outdir = make_outdir_name(\n",
    "        data_dir, \n",
    "        utils.get_model_name(chosen_model), \n",
    "        chosen_optimizer.__name__,\n",
    "        str(iteration),\n",
    "        prepend=EXPERIMENTS_DIR\n",
    "    )\n",
    "    \n",
    "    # make our data loaders based on the image size\n",
    "    image_size = utils.determine_image_size(utils.get_model_name(chosen_model))\n",
    "    # get_train_val_dataloaders() makes a stratified random partitions of a train and validation set\n",
    "    dataloaders = utils.get_train_val_dataloaders(\n",
    "        datadir=data_dir,\n",
    "        val_proportion=0.15,\n",
    "        image_size=image_size, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    # build our model\n",
    "    model = build_model(chosen_model)\n",
    "    utils.fit_model_last_to_dataset(model, dataloaders['train'].dataset)\n",
    "    # build the optimizer\n",
    "    optimizer = chosen_optimizer(model.parameters(), *optim_params)\n",
    "    # \"ReduceOnPlateau\" is \"dev-decay\" as recommended by Wilson et al. 2018 \n",
    "    # \"The Marginal Value of Adaptive Gradient Methods\"\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=lr_factor, \n",
    "        patience=1\n",
    "    )\n",
    "    \n",
    "    trainable = Trainable(dataloaders, model, CRITERION, optimizer, lr_scheduler, outdir=outdir)\n",
    "    trainable.train(num_epochs=NUM_TRAIN_EPOCHS, early_stop_limit=EARLY_STOP, verbose=False)\n",
    "\n",
    "    return trainable\n",
    "\n",
    "\n",
    "def parse_train_params(params):\n",
    "    \"\"\"\n",
    "    Parses train parameters by converting batch size to an int, and the betas to a tuple for Adam.\n",
    "    \"\"\"\n",
    "    batch_size, lr_factor, optim_params = int(params[0]), params[1], params[2:]\n",
    "    if chosen_optimizer is torch.optim.Adam:  # turn b1 and b2 into a tuple\n",
    "        # NOTE the below indexes are based on purely ADAM_DOMAin\n",
    "        optim_params = (optim_params[0], tuple(optim_params[1:3]), optim_params[3])\n",
    "#         print('OPTIM_PARAMS', optim_params)\n",
    "    return batch_size, lr_factor, optim_params\n",
    "\n",
    "    \n",
    "def build_model(model_fn):\n",
    "    \"\"\"\n",
    "    Build a pretrained model class from a model function. Passes \n",
    "    in the appropriate pretrained arg based on the model function's \n",
    "    parent module.\n",
    "    \"\"\"\n",
    "    if 'pretrainedmodels' in model_fn.__module__:\n",
    "        model = model_fn(num_classes=1000, pretrained='imagenet')\n",
    "    else:\n",
    "        model = model_fn(pretrained=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_key_from_params(params):\n",
    "    \"\"\"\n",
    "    Makes a unique key (as a tuple) from a given list of parameters.\n",
    "    For storing associated Trainable objects.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tuple(round(param, 10) for param in params)\n",
    "\n",
    "\n",
    "def make_outdir_name(data_dir, *append, prepend=\"\"):\n",
    "    \"\"\"\n",
    "    Make the output directory name based on dataset, model name, and any extra info.\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.basename(data_dir)\n",
    "    return os.path.join(prepend, dataset_name, *append) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_DOMAIN = [\n",
    "#     {'name': 'batch_size', 'type': 'discrete', 'domain': (1, 4, 8, 12)},  # DEBUG for GPUs with insufficient memory\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 24, 32, 48, 64)},\n",
    "    {'name': 'lr_decay', 'type': 'continuous', 'domain': (0.03, 0.3)},\n",
    "]\n",
    "\n",
    "ADAM_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'adam_lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'adam_beta1', 'type': 'continuous', 'domain': (0.8, .99)},\n",
    "    {'name': 'adam_beta2', 'type': 'continuous', 'domain': (0.95, .9999)},\n",
    "    {'name': 'adam_wtdecay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]\n",
    "# TODO: have to figure out how to set a starting default\n",
    "# default_input = [32, 0.001, 0.9, 0.999, 0] \n",
    "\n",
    "SGD_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'momentum', 'type': 'continuous', 'domain': (0.5, .99)},\n",
    "    {'name': 'weight_decay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" Value function to maximize for bayesian optimization \"\"\"\n",
    "    params = x.flatten()\n",
    "#     print('PARAMS', params)\n",
    "    \n",
    "    trainable = train(params)\n",
    "    val_acc = trainable.best_val_accuracy\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do BO on all models on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "from predict import create_predictions\n",
    "from metrics import create_all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BO helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_globals(datadir):\n",
    "    \"\"\"\n",
    "    Reset the iteration and set the data directory\n",
    "    \"\"\"\n",
    "    global iteration  # keep track of our optimization iterations for directory output\n",
    "    iteration = 0   # but reset to 0 each train run\n",
    "    global data_dir\n",
    "    data_dir = datadir\n",
    "    \n",
    "\n",
    "def get_domain():\n",
    "    \"\"\"\n",
    "    Get the domain parameters given the chosen optimizer\n",
    "    \"\"\"\n",
    "    return ADAM_DOMAIN if chosen_optimizer is torch.optim.Adam else SGD_DOMAIN\n",
    "    \n",
    "def perform_bayesian_optimization():\n",
    "    \"\"\"\n",
    "    Construct the problem and run the optimization.\n",
    "    \"\"\"\n",
    "    domain = get_domain()\n",
    "    problem = BayesianOptimization(\n",
    "        f=f,\n",
    "        domain=domain,\n",
    "        maximize=True\n",
    "    )\n",
    "    problem.run_optimization(max_iter=MAX_ITERATIONS)\n",
    "    return problem\n",
    "\n",
    "def plot_bo_results(problem):\n",
    "    \"\"\"\n",
    "    Graph the acquisition function and convergence\n",
    "    \"\"\"\n",
    "    print('Best params:', problem.opt_x)\n",
    "    problem.plot_acquisition()\n",
    "    problem.plot_convergence()\n",
    "    \n",
    "    \n",
    "def params_to_meta_dict(params):\n",
    "    \"\"\"\n",
    "    Takes a list and returns a dictionary of named parameters based on domain index\n",
    "    \"\"\"\n",
    "    domain = get_domain()\n",
    "    meta = { d['name']: params[i] for i, d in enumerate(domain)}\n",
    "    return meta\n",
    "\n",
    "\n",
    "def generate_test_metrics(trainable):\n",
    "    \"\"\"\n",
    "    Create an itemized predictions file and metrics for the test set.\n",
    "    \"\"\"\n",
    "    predictions_file = create_predictions(\n",
    "        outdir=trainable.outdir,\n",
    "        subset='test',\n",
    "        data_dir=data_dir,\n",
    "        model=best_trainable.model\n",
    "    )\n",
    "    create_all_metrics(predictions_file, trainable.outdir, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet Adam data/die_vs_all_tt; Iteration: 0, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train, best val=0.566667:  57%|█████▋    | 576/1018 [00:08<00:06, 66.97images/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.\n",
      "Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.\n",
      "Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigint caught!\n",
      "Training will stop after this epoch and the best model so far will be saved.\n",
      "OR press Ctrl-C again to quit immediately without saving.\n",
      "Stopping...\n",
      "Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping...\n",
      "Stopping...\n",
      "Stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/alwood/anaconda3/envs/eai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2969: UserWarning:To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import gc # for manual garbage collection, reduce memory consumption\n",
    "\n",
    "for chosen_optimizer in OPTIMIZERS:\n",
    "    for chosen_model in MODELS:  # iterate over all models\n",
    "\n",
    "        # iterate over both binary and quaternary datasets\n",
    "        for data_dir in (\n",
    "            os.path.join('data', 'die_vs_all_tt'), \n",
    "            os.path.join('data', '4_class_tt')\n",
    "        ):\n",
    "            print(utils.get_model_name(chosen_model), chosen_optimizer.__name__, data_dir, end=\"; \")\n",
    "            reset_globals(data_dir)  # reset some globals used for iteration tracking\n",
    "\n",
    "            try:\n",
    "                # define and optimize the problem\n",
    "                optimized = perform_bayesian_optimization()\n",
    "                # plot the results\n",
    "                plot_bo_results(optimized)\n",
    "                # get and save the best trainable\n",
    "                best_params = optimized.x_opt.flatten()\n",
    "                print('Best params:' (params_to_meta_dict(params)))\n",
    "                best_trainable = train(best_params)\n",
    "                best_trainable.save(extra_meta=params_to_meta_dict(best_params))\n",
    "                # evalute on the test set using the best model\n",
    "                generate_test_metrics(best_trainable)\n",
    "\n",
    "                gc.collect()  # reduce memory usage\n",
    "\n",
    "            # if something bad happens, skip it so we can let the others run\n",
    "            except Exception as e:\n",
    "                print('Skipping because', e)\n",
    "#                 import traceback  # DEBUG\n",
    "#                 traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "            # commit & push only if we can connect to internet\n",
    "            if PUSH_TO_GIT:\n",
    "                subprocess.check_call(['git', 'add', 'experiments'])\n",
    "                subprocess.check_call(['git', 'commit', '-am', \n",
    "                                       \"Results from \" +\n",
    "                                       f'{data_dir} {utils.get_model_name(chosen_model)} ' + \n",
    "                                       f'{chosen_optimizer.__name__}'])\n",
    "                subprocess.check_call(['git', 'push'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Commit and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_TO_GIT:\n",
    "    import time\n",
    "    time.sleep(120) # wait for two minutes to let everything rendering\n",
    "    _ = subprocess.check_call([\"spd-say\", \"Your code has finished running\"])\n",
    "    _ = subprocess.check_call(['git', 'commit', '-am', \"BO final commit\"])\n",
    "    _ = subprocess.check_call(['git', 'push'])\n",
    "    \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
