{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# whether to commit and push to git after each optimization. intended for long runs\n",
    "PUSH_TO_GIT = False  \n",
    "\n",
    "# parent output directory\n",
    "EXPERIMENTS_DIR = os.path.join(\"experiments\", \"bayes_opt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer to use. Choose by commenting. (Intent is to use train different ones on differen t machines for efficiency.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CHOSEN_OPTIMIZER = torch.optim.SGD\n",
    "CHOSEN_OPTIMIZER = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of iterations for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to test for BO. All in the list will be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as tvm\n",
    "import pretrainedmodels as ptm\n",
    "\n",
    "# The models we will test\n",
    "MODELS = (\n",
    "    ptm.alexnet, # gets maximum recursion limit exceeded exceptions\n",
    "    ptm.se_resnet50,\n",
    "    ptm.se_resnet101,\n",
    "    ptm.inceptionresnetv2,\n",
    "    ptm.inceptionv4,\n",
    "    ptm.vgg16,\n",
    "    ptm.vgg19,\n",
    "    tvm.resnet101,\n",
    "    ptm.senet154,\n",
    "    ptm.nasnetalarge\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup: Make sure Jupyter shows all output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show more than one output in cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# plot charts in our notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "sys.setrecursionlimit(1500) # for some reason AlexNet requires more recursive depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import src.utils as utils\n",
    "from src.trainable import Trainable\n",
    "import torch\n",
    "\n",
    "\n",
    "CRITERION = torch.nn.CrossEntropyLoss() # we'll always use CE for the loss function\n",
    " \n",
    "\n",
    "def train(batch_size, adam_lr, adam_b1, adam_b2, adam_wtdecay):\n",
    "    \"\"\"\n",
    "    Set up a trainable and train it using the given parameters.\n",
    "    \"\"\"\n",
    "    input_params = locals()  # the parameters will become the key\n",
    "    \n",
    "    # make an output directory using the model, dataset, and BO iteration\n",
    "    outdir = make_outdir_name(data_dir, utils.get_model_name_from_fn(chosen_model), \n",
    "                              prepend=EXPERIMENTS_DIR,\n",
    "                              append=str(iteration))\n",
    "    \n",
    "    \n",
    "    image_size = utils.determine_image_size(utils.get_model_name_from_fn(chosen_model))\n",
    "    dataloaders = utils.get_train_val_dataloaders(\n",
    "        datadir=data_dir,\n",
    "        val_proportion=0.15,\n",
    "        image_size=image_size, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model = build_model(chosen_model)\n",
    "    utils.fit_model_last_to_dataset(model, dataloaders['train'].dataset)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 adam_lr, (adam_b1, adam_b2), adam_wtdecay)\n",
    "    \n",
    "    trainable = Trainable(dataloaders, model, CRITERION, optimizer, outdir=outdir)\n",
    "    acc = trainable.train(50, early_stop_limit=6, verbose=False)\n",
    "    \n",
    "    # store this train iteration in a dictionary for later\n",
    "    global trainables\n",
    "    key = make_key_from_params(input_params)\n",
    "    trainables.append({key: trainable})\n",
    "    return acc\n",
    "    \n",
    "    \n",
    "def build_model(model_fn):\n",
    "    \"\"\"\n",
    "    Build a pretrained model class from a model function. Passes \n",
    "    in the appropriate pretrained arg based on the model function's \n",
    "    parent module.\n",
    "    \"\"\"\n",
    "    print(model_fn)\n",
    "    if 'pretrainedmodels' in model_fn.__module__:\n",
    "        model = model_fn(num_classes=1000, pretrained='imagenet')\n",
    "    else:\n",
    "        model = model_fn(pretrained=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_key_from_params(params):\n",
    "    \"\"\"\n",
    "    Makes a unique key (as a tuple) from a given list of parameters.\n",
    "    For storing associated Trainable objects.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tuple(round(param, 10) for param in params)\n",
    "\n",
    "\n",
    "def make_outdir_name(datadir, model_name, prepend=\"\", append=\"\"):\n",
    "    \"\"\"\n",
    "    Make the output directory name based on dataset, model name, and any extra info.\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.basename(datadir)\n",
    "    return os.path.join(prepend, dataset_name, model_name, append) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_DOMAIN = [{'name': 'batch_size', 'type': 'discrete', 'domain': (16, 24, 32, 48, 64)},]\n",
    "\n",
    "ADAM_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'adam_lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'adam_beta1', 'type': 'continuous', 'domain': (0.8, .99)},\n",
    "    {'name': 'adam_beta2', 'type': 'continuous', 'domain': (0.95, .9999)},\n",
    "    {'name': 'adam_wtdecay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]\n",
    "# TODO: have to figure out how to set a startingdefault\n",
    "#default_input = [32, 0.001, 0.9, 0.999, 0] \n",
    "\n",
    "SGD_DOMAIN = BASE_DOMAIN + [\n",
    "    {'name': 'lr', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "    {'name': 'momentum', 'type': 'continuous', 'domain': (0.5, .99)},\n",
    "    {'name': 'weight_decay', 'type': 'continuous', 'domain': (0, 1)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\" Value function to maximize for bayesian optimization \"\"\"\n",
    "    batch_size = int(x[:, 0])  # the first arg is always batch size\n",
    "    args = x[:, 1:]  # the remaining depend on if we're using Adam or SGD\n",
    "    \n",
    "    val_acc = train(batch_size=batch_size, *args)\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do BO on all models on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "from predict import create_predictions\n",
    "from metrics import create_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_globals(data_dir):\n",
    "    global iteration  # keep track of our optimization iterations for directory output\n",
    "    iteration = 0   # but reset to 0 each train run\n",
    "    global data_dir\n",
    "    data_dir = data_dir\n",
    "    \n",
    "    # reset the global trainables produced by BO\n",
    "    global trainables\n",
    "    trainables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bayesian_optimization():\n",
    "    \"\"\"\n",
    "    Construct the problem and run the optimization.\n",
    "    \"\"\"\n",
    "    domain = ADAM_DOMAIN if CHOSEN_OPTIMIZER is torch.optim.Adam else SGD_DOMAIN\n",
    "    problem = BayesianOptimization(\n",
    "        f=f,\n",
    "        domain=domain,\n",
    "        maximize=True\n",
    "    )\n",
    "    problem.run_optimization(max_iter=MAX_ITERATIONS)\n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bo_results(problem):\n",
    "    \"\"\"\n",
    "    Graph the acquisition function and convergence\n",
    "    \"\"\"\n",
    "    problem.plot_acquisition()\n",
    "    problem.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_trainable(problem):\n",
    "    \"\"\"\n",
    "    Get the best trainable, based on the optimized parameters, from the global \n",
    "    stored trainables.\n",
    "    \"\"\"\n",
    "    best_params = problem.x_opt\n",
    "    key = make_key_from_params(best_params)\n",
    "    best_trainable = trainables[key]\n",
    "    return best_trainable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_metrics(trainable):\n",
    "    \"\"\"\n",
    "    Create an itemized predictions file and metrics for the test set.\n",
    "    \"\"\"\n",
    "    predictions_file = create_predictions(\n",
    "        outdir=trainable.outdir,\n",
    "        subset='test',\n",
    "        data_dir=data_dir,\n",
    "        model=best_trainable.model\n",
    "    )\n",
    "    create_all_metrics(predictions_file, trainable.outdir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_fn in MODELS:  # iterate over all models\n",
    "    # set the model\n",
    "    global chosen_model\n",
    "    chosen_model = model_fn\n",
    "    \n",
    "    # iterate over both binary and quaternary datasets\n",
    "    for data_index, data_dir in enumerate(\n",
    "        (os.path.join('data/die_vs_all_tt'), \n",
    "        os.path.join('data/4_class_tt'))\n",
    "    ):\n",
    "        reset_globals(data_dir)  # reset some globals used for iteration tracking\n",
    "        \n",
    "        try:\n",
    "            # define and optimize the problem\n",
    "            optimized = perform_bayesian_optimization()\n",
    "            # plot the results\n",
    "            plot_bo_results(optimized)\n",
    "            # get and save the best trainable\n",
    "            best_trainable = get_best_trainable(optimized)\n",
    "            best_trainable.save()\n",
    "            # evalute on the test set using the best model\n",
    "            generate_test_metrics(best_trainable)\n",
    "            \n",
    "        # if something bad happens, skip it so we can let the others run\n",
    "        except Exception as e:\n",
    "            print('Skipping because', e)\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "            continue\n",
    "            \n",
    "        # commit & push only if we can connect to internet\n",
    "        if PUSH_TO_GIT:\n",
    "            subprocess.check_call(['git', 'add', 'experiments'])\n",
    "            subprocess.check_call(['git', 'commit', '-am', f'Results from {str(chosen_model).split()[1]} {data_dir}'])\n",
    "            subprocess.check_call(['git', 'push'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Commit and Push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_TO_GIT:\n",
    "    import time\n",
    "    time.sleep(120) # wait for two minutes to let everything rendering\n",
    "    _ = subprocess.check_call([\"spd-say\", \"Your code has finished running\"])\n",
    "    _ = subprocess.check_call(['git', 'commit', '-am', \"BO final commit\"])\n",
    "    _ = subprocess.check_call(['git', 'push'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
