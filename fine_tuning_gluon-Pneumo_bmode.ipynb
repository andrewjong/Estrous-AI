{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon.data.vision.datasets import ImageFolderDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import mxnet.contrib.onnx as onnx_mxnet\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile, os\n",
    "import json\n",
    "import multiprocessing\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:images/wrench.jpg exists, skipping download\n",
      "INFO:root:images/dolphin.jpg exists, skipping download\n",
      "INFO:root:images/lotus.jpg exists, skipping download\n",
      "INFO:root:utils.py exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"images\"\n",
    "utils_file = \"utils.py\" # contain utils function to plot nice visualization\n",
    "images = ['wrench.jpg', 'dolphin.jpg', 'lotus.jpg']\n",
    "base_url = \"https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/doc/tutorials/onnx/{}?raw=true\"\n",
    "\n",
    "\n",
    "for image in images:\n",
    "    mx.test_utils.download(base_url.format(\"{}/{}\".format(image_folder, image)), fname=image,dirname=image_folder)\n",
    "mx.test_utils.download(base_url.format(utils_file), fname=utils_file)\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading a model from the ONNX model zoo\n",
    "\n",
    "We download a pre-trained model, in our case the [GoogleNet](https://arxiv.org/abs/1409.4842) model, trained on [ImageNet](http://www.image-net.org/) from the [ONNX model zoo](https://github.com/onnx/models). The model comes packaged in an archive `tar.gz` file containing an `model.onnx` model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://s3.amazonaws.com/download.onnx/models/opset_3/\"\n",
    "current_model = \"bvlc_googlenet\"\n",
    "model_folder = \"/home/ubuntu/pneumothorax/model\"\n",
    "archive_file = \"{}.tar.gz\".format(current_model)\n",
    "archive_path = os.path.join(model_folder, archive_file)\n",
    "url = \"{}{}\".format(base_url, archive_file)\n",
    "onnx_path = os.path.join(model_folder, current_model, 'model.onnx')\n",
    "\n",
    "'''\n",
    "# Download the zipped model\n",
    "mx.test_utils.download(url, dirname = model_folder)\n",
    "'''\n",
    "\n",
    "# Extract the model\n",
    "if not os.path.isdir(os.path.join(model_folder, current_model)):\n",
    "    print('Extracting {} in {}...'.format(archive_path, model_folder))\n",
    "    tar = tarfile.open(archive_path, \"r:gz\")\n",
    "    tar.extractall(model_folder)\n",
    "    tar.close()\n",
    "    print('Model extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/ubuntu/pneumothorax/mmode\"\n",
    "dataset_name = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = os.path.join(data_folder, dataset_name)\n",
    "testing_path = os.path.join(data_folder, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data using an ImageFolderDataset and a DataLoader\n",
    "\n",
    "We need to transform the images to a format accepted by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE = 224\n",
    "SIZE = (EDGE, EDGE)\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, label):\n",
    "    resized = mx.image.resize_short(image, EDGE)\n",
    "    cropped, crop_info = mx.image.center_crop(resized, SIZE)\n",
    "    transposed = nd.transpose(cropped, (2,0,1))\n",
    "    return transposed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ImageFolderDataset(root=training_path, transform=transform)\n",
    "dataset_test = ImageFolderDataset(root=testing_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 1829 images, Test dataset: 457 images\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, last_batch='discard',\n",
    "                              shuffle=True, num_workers=NUM_WORKERS)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, last_batch='discard',\n",
    "                             shuffle=True, num_workers=NUM_WORKERS)\n",
    "print(\"Train dataset: {} images, Test dataset: {} images\".format(len(dataset_train), len(dataset_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset_train.synsets\n",
    "NUM_CLASSES = len(categories)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the ONNX model\n",
    "\n",
    "### Getting the last layer\n",
    "\n",
    "Load the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym, arg_params, aux_params = onnx_mxnet.import_model(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function get the output of a given layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output(symbol, arg_params, aux_params, layer_name):\n",
    "    all_layers = symbol.get_internals()\n",
    "    net = all_layers[layer_name+'_output']\n",
    "    net = mx.symbol.Flatten(data=net)\n",
    "    new_args = dict({k:arg_params[k] for k in arg_params if k in net.list_arguments()})\n",
    "    new_aux = dict({k:aux_params[k] for k in aux_params if k in net.list_arguments()})\n",
    "    return (net, new_args, new_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Symbol group [data_0, pad0, conv1/7x7_s2_w_0, conv1/7x7_s2_b_0, convolution0, relu0, pad1, pooling0, lrn0, pad2, conv2/3x3_reduce_w_0, conv2/3x3_reduce_b_0, convolution1, relu1, pad3, conv2/3x3_w_0, conv2/3x3_b_0, convolution2, relu2, lrn1, pad4, pooling1, pad5, inception_3a/1x1_w_0, inception_3a/1x1_b_0, convolution3, relu3, pad6, inception_3a/3x3_reduce_w_0, inception_3a/3x3_reduce_b_0, convolution4, relu4, pad7, inception_3a/3x3_w_0, inception_3a/3x3_b_0, convolution5, relu5, pad8, inception_3a/5x5_reduce_w_0, inception_3a/5x5_reduce_b_0, convolution6, relu6, pad9, inception_3a/5x5_w_0, inception_3a/5x5_b_0, convolution7, relu7, pad10, pooling2, pad11, inception_3a/pool_proj_w_0, inception_3a/pool_proj_b_0, convolution8, relu8, concat0, pad12, inception_3b/1x1_w_0, inception_3b/1x1_b_0, convolution9, relu9, pad13, inception_3b/3x3_reduce_w_0, inception_3b/3x3_reduce_b_0, convolution10, relu10, pad14, inception_3b/3x3_w_0, inception_3b/3x3_b_0, convolution11, relu11, pad15, inception_3b/5x5_reduce_w_0, inception_3b/5x5_reduce_b_0, convolution12, relu12, pad16, inception_3b/5x5_w_0, inception_3b/5x5_b_0, convolution13, relu13, pad17, pooling3, pad18, inception_3b/pool_proj_w_0, inception_3b/pool_proj_b_0, convolution14, relu14, concat1, pad19, pooling4, pad20, inception_4a/1x1_w_0, inception_4a/1x1_b_0, convolution15, relu15, pad21, inception_4a/3x3_reduce_w_0, inception_4a/3x3_reduce_b_0, convolution16, relu16, pad22, inception_4a/3x3_w_0, inception_4a/3x3_b_0, convolution17, relu17, pad23, inception_4a/5x5_reduce_w_0, inception_4a/5x5_reduce_b_0, convolution18, relu18, pad24, inception_4a/5x5_w_0, inception_4a/5x5_b_0, convolution19, relu19, pad25, pooling5, pad26, inception_4a/pool_proj_w_0, inception_4a/pool_proj_b_0, convolution20, relu20, concat2, pad27, inception_4b/1x1_w_0, inception_4b/1x1_b_0, convolution21, relu21, pad28, inception_4b/3x3_reduce_w_0, inception_4b/3x3_reduce_b_0, convolution22, relu22, pad29, inception_4b/3x3_w_0, inception_4b/3x3_b_0, convolution23, relu23, pad30, inception_4b/5x5_reduce_w_0, inception_4b/5x5_reduce_b_0, convolution24, relu24, pad31, inception_4b/5x5_w_0, inception_4b/5x5_b_0, convolution25, relu25, pad32, pooling6, pad33, inception_4b/pool_proj_w_0, inception_4b/pool_proj_b_0, convolution26, relu26, concat3, pad34, inception_4c/1x1_w_0, inception_4c/1x1_b_0, convolution27, relu27, pad35, inception_4c/3x3_reduce_w_0, inception_4c/3x3_reduce_b_0, convolution28, relu28, pad36, inception_4c/3x3_w_0, inception_4c/3x3_b_0, convolution29, relu29, pad37, inception_4c/5x5_reduce_w_0, inception_4c/5x5_reduce_b_0, convolution30, relu30, pad38, inception_4c/5x5_w_0, inception_4c/5x5_b_0, convolution31, relu31, pad39, pooling7, pad40, inception_4c/pool_proj_w_0, inception_4c/pool_proj_b_0, convolution32, relu32, concat4, pad41, inception_4d/1x1_w_0, inception_4d/1x1_b_0, convolution33, relu33, pad42, inception_4d/3x3_reduce_w_0, inception_4d/3x3_reduce_b_0, convolution34, relu34, pad43, inception_4d/3x3_w_0, inception_4d/3x3_b_0, convolution35, relu35, pad44, inception_4d/5x5_reduce_w_0, inception_4d/5x5_reduce_b_0, convolution36, relu36, pad45, inception_4d/5x5_w_0, inception_4d/5x5_b_0, convolution37, relu37, pad46, pooling8, pad47, inception_4d/pool_proj_w_0, inception_4d/pool_proj_b_0, convolution38, relu38, concat5, pad48, inception_4e/1x1_w_0, inception_4e/1x1_b_0, convolution39, relu39, pad49, inception_4e/3x3_reduce_w_0, inception_4e/3x3_reduce_b_0, convolution40, relu40, pad50, inception_4e/3x3_w_0, inception_4e/3x3_b_0, convolution41, relu41, pad51, inception_4e/5x5_reduce_w_0, inception_4e/5x5_reduce_b_0, convolution42, relu42, pad52, inception_4e/5x5_w_0, inception_4e/5x5_b_0, convolution43, relu43, pad53, pooling9, pad54, inception_4e/pool_proj_w_0, inception_4e/pool_proj_b_0, convolution44, relu44, concat6, pad55, pooling10, pad56, inception_5a/1x1_w_0, inception_5a/1x1_b_0, convolution45, relu45, pad57, inception_5a/3x3_reduce_w_0, inception_5a/3x3_reduce_b_0, convolution46, relu46, pad58, inception_5a/3x3_w_0, inception_5a/3x3_b_0, convolution47, relu47, pad59, inception_5a/5x5_reduce_w_0, inception_5a/5x5_reduce_b_0, convolution48, relu48, pad60, inception_5a/5x5_w_0, inception_5a/5x5_b_0, convolution49, relu49, pad61, pooling11, pad62, inception_5a/pool_proj_w_0, inception_5a/pool_proj_b_0, convolution50, relu50, concat7, pad63, inception_5b/1x1_w_0, inception_5b/1x1_b_0, convolution51, relu51, pad64, inception_5b/3x3_reduce_w_0, inception_5b/3x3_reduce_b_0, convolution52, relu52, pad65, inception_5b/3x3_w_0, inception_5b/3x3_b_0, convolution53, relu53, pad66, inception_5b/5x5_reduce_w_0, inception_5b/5x5_reduce_b_0, convolution54, relu54, pad67, inception_5b/5x5_w_0, inception_5b/5x5_b_0, convolution55, relu55, pad68, pooling12, pad69, inception_5b/pool_proj_w_0, inception_5b/pool_proj_b_0, convolution56, relu56, concat8, pad70, pooling13, dropout0, flatten0, loss3/classifier_w_0, linalg_gemm20, loss3/classifier_b_0, _mulscalar0, broadcast_add0, softmax0]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym.get_internals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the network until the output of the `flatten0` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sym, new_arg_params, new_aux_params = get_layer_output(sym, arg_params, aux_params, 'flatten0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning in gluon\n",
    "\n",
    "\n",
    "We can now take advantage of the features and pattern detection knowledge that our network learnt training on ImageNet, and apply that to the new Caltech101 dataset.\n",
    "\n",
    "\n",
    "We pick a context, fine-tuning on CPU will be **WAY** slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a symbol block that is going to hold all our pre-trained layers, and assign the weights of the different pre-trained layers to the newly created SymbolBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained = gluon.nn.SymbolBlock(outputs=new_sym, inputs=mx.sym.var('data_0'))\n",
    "net_params = pre_trained.collect_params()\n",
    "for param in new_arg_params:\n",
    "    if param in net_params:\n",
    "        net_params[param]._load_init(new_arg_params[param], ctx=ctx)\n",
    "for param in new_aux_params:\n",
    "    if param in net_params:\n",
    "        net_params[param]._load_init(new_aux_params[param], ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = gluon.nn.Dense(NUM_CLASSES, activation='relu')\n",
    "dense_layer.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnorm = gluon.nn.BatchNorm()\n",
    "bnorm.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the SymbolBlock and the new dense layer to a HybridSequential network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "net.add(pre_trained)\n",
    "net.add(dense_layer)\n",
    "net.add(bnorm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "Softmax cross entropy for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "Initialize trainer with common training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "WDECAY = 0.00001\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer will retrain and fine-tune the entire network. If we use `dense_layer` instead of `net` in the cell below, the gradient updates would only be applied to the new last dense layer. Essentially we would be using the pre-trained network as a featurizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'rmsprop', \n",
    "                        {'learning_rate': LEARNING_RATE,\n",
    "                         'wd':WDECAY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation loop\n",
    "\n",
    "We measure the accuracy in a non-blocking way, using `nd.array` to take care of the parallelisation that MXNet and Gluon offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate_accuracy_gluon(data_iterator, net):\n",
    "    num_instance = nd.zeros(1, ctx=ctx)\n",
    "    sum_metric = nd.zeros(1,ctx=ctx, dtype=np.int32)\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.astype(np.float32).as_in_context(ctx)\n",
    "        label = label.astype(np.int32).as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1).astype(np.int32)\n",
    "        num_instance += len(prediction)\n",
    "        sum_metric += (prediction==label).sum()\n",
    "    accuracy = (sum_metric.astype(np.float32)/num_instance.astype(np.float32))\n",
    "    return accuracy.asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained network Test Accuracy: 0.4353\n",
      "CPU times: user 1.88 s, sys: 4.07 s, total: 5.96 s\n",
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Untrained network Test Accuracy: {0:.4f}\".format(evaluate_accuracy_gluon(dataloader_test, net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] Test Accuracy 0.9308 \n",
      "Epoch [1] Test Accuracy 0.9152 \n",
      "Epoch [2] Test Accuracy 0.9018 \n",
      "Epoch [3] Test Accuracy 0.9643 \n",
      "Epoch [4] Test Accuracy 0.9442 \n",
      "Epoch [5] Test Accuracy 0.9754 \n",
      "Epoch [6] Test Accuracy 0.9777 \n",
      "Epoch [7] Test Accuracy 0.9844 \n",
      "Epoch [8] Test Accuracy 0.9777 \n",
      "Epoch [9] Test Accuracy 0.9821 \n",
      "Epoch [10] Test Accuracy 0.9821 \n",
      "Epoch [11] Test Accuracy 0.9799 \n",
      "Epoch [12] Test Accuracy 0.9799 \n",
      "Validation accuracy is not improving, stopping training\n"
     ]
    }
   ],
   "source": [
    "max_val_accuracy = 0\n",
    "count = 0\n",
    "n_count_stop = 5\n",
    "for epoch in range(100):\n",
    "    for i, (data, label) in enumerate(dataloader_train):\n",
    "        data = data.astype(np.float32).as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "\n",
    "        if i%100==0 and i >0:\n",
    "            print('Batch [{0}] loss: {1:.4f}'.format(i, loss.mean().asscalar()))\n",
    "\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "    nd.waitall() # wait at the end of the epoch    \n",
    "    new_val_accuracy = evaluate_accuracy_gluon(dataloader_test, net)    \n",
    "    print(\"Epoch [{0}] Test Accuracy {1:.4f} \".format(epoch, new_val_accuracy))\n",
    "\n",
    "    # We perform early-stopping regularization, to prevent the model from overfitting\n",
    "    if max_val_accuracy >= new_val_accuracy:\n",
    "        count +=1\n",
    "        if count == n_count_stop:\n",
    "            print('Validation accuracy is not improving, stopping training')\n",
    "            break\n",
    "    else:\n",
    "        count = 0\n",
    "        max_val_accuracy = new_val_accuracy              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "display_name": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
